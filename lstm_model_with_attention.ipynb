{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "626594"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "CORPUS_PATH = \"livedoor_data/text/sports-watch/*.txt\"\n",
    "text_corpus = glob(CORPUS_PATH)\n",
    "start_token = \"<s>\"\n",
    "end_token = \"</s>\"\n",
    "entire_text = \"\"\n",
    "\n",
    "for filepath in text_corpus:\n",
    "    if filepath == \"livedoor_data/text/sports-watch/LICENSE.txt\":\n",
    "        continue\n",
    "    else:\n",
    "        tmp_text = \"\" + start_token\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            #skip first 2 rows for each document\n",
    "            #1st row:referred URL, 2nd row:the article-written date\n",
    "            for i in range(2):\n",
    "                next(f)\n",
    "            tmp_text += f.read()\n",
    "            entire_text += tmp_text\n",
    "            entire_text += end_token\n",
    "        \n",
    "len(entire_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '<', 's', '>', '【', 'S', 'p', 'or', 't', 's', '▁W', 'at', 'ch', '】', '秋', '山', '成', '勲', '、', 'メール', 'で', '吉田', 'に', '対戦', '迫', 'った', '!', '?', '▁', '今', '月', '8', '日', '、', '都', '内', 'ホテル', 'では', '、', '総合', '格闘', '家', '・', '吉田', '秀', '彦', 'の', '引退', '試合', '興行', '「', 'A', 'ST', 'RA', '」', 'の', '開催', 'が', '発表された', '。', '▁', 'バル', 'セ', 'ロ', 'ナ', '五', '輪', '柔道', '金', 'メ', 'ダ', 'リスト', 'としての', '実', '績', 'を', '引', 'っ', 'さ', 'げ', '、', '2002', '年に', 'プロ', '総合', '格闘', '家', 'に転', '向', '。', '以後', '、', '数', '々', 'の', '死', '闘', 'を', '繰り', '広']\n",
      "length of str_tokens: 423332\n",
      "[6, 2003, 160, 1954, 6050, 130, 334, 531, 268, 160, 1677, 1055, 945, 6051, 1341, 63, 391, 5563, 3, 4893, 14, 4366, 10, 2789, 2840, 105, 947, 2607, 6, 702, 19, 62, 30, 3, 949, 155, 2839, 36, 3, 1498, 5311, 122, 11, 4366, 2046, 2299, 4, 1583, 569, 4972, 20, 141, 1997, 2774, 18, 4, 814, 9, 2508, 5, 6, 1077, 266, 83, 90, 810, 1216, 5470, 220, 188, 246, 1610, 1764, 291, 4879, 8, 1301, 360, 202, 862, 3, 1386, 72, 524, 1498, 5311, 122, 3469, 613, 5, 1816, 3, 158, 502, 4, 519, 2443, 8, 4461, 466]\n",
      "length of int_tokens: 423332\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "tokenizer = spm.SentencePieceProcessor()\n",
    "tokenizer.Load(\"wiki_data/wikiextractor/spm.model\")\n",
    "\n",
    "str_tokens = tokenizer.EncodeAsPieces(entire_text)\n",
    "print(str_tokens[:100])\n",
    "print(\"length of str_tokens:\", len(str_tokens))\n",
    "\n",
    "int_tokens = []\n",
    "for token in str_tokens:\n",
    "    int_tokens.append(tokenizer.piece_to_id(token))\n",
    "    \n",
    "print(int_tokens[:100])\n",
    "print(\"length of int_tokens:\", len(int_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((423312, 20), (423312, 8000))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "input_texts, target_texts = [], []\n",
    "seq_length = 20\n",
    "num_vocabs = 8000\n",
    "\n",
    "for i in range(0, len(int_tokens) - seq_length, 1):\n",
    "    input_texts.append(int_tokens[i: i + seq_length])\n",
    "    target_texts.append(int_tokens[i + seq_length])\n",
    "    \n",
    "target_texts_one_hot = to_categorical(target_texts, num_classes=num_vocabs)\n",
    "X = np.array(input_texts)\n",
    "y = np.array(target_texts_one_hot)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 300)    2400000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)        (None, 256)          571392      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         cu_dnnlstm_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1)            0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 256, 1)       0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 1, 256)       0           repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 1, 256)       0           cu_dnnlstm_1[0][0]               \n",
      "                                                                 permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 256)          0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 8000)         2056000     lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 5,027,649\n",
      "Trainable params: 5,027,649\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, CuDNNLSTM, Dropout, Dense, Reshape, Activation, Permute, Multiply, Lambda, RepeatVector\n",
    "import keras.backend as K\n",
    "\n",
    "embed_dims = 300\n",
    "hidden_dims = 256\n",
    "\n",
    "input_text = Input((None,))\n",
    "x = Embedding(num_vocabs, embed_dims)(input_text)\n",
    "x = CuDNNLSTM(hidden_dims)(x)\n",
    "#shape of x is (seq_length, hidden_dims)\n",
    "##attention part\n",
    "e = Dense(1, activation=\"tanh\")(x)\n",
    "e = Reshape([-1])(e)\n",
    "alpha = Activation(\"softmax\")(e)\n",
    "\"\"\"\n",
    "Explanation of Permute\n",
    "model = Sequential()\n",
    "model.add(Permute((2, 1), input_shape=(10, 64)))\n",
    "# now: model.output_shape == (None, 64, 10)\n",
    "# note: `None` is the batch dimension\n",
    "\n",
    "Explanation of RepeatVector\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=32))\n",
    "# now: model.output_shape == (None, 32)\n",
    "# note: `None` is the batch dimension\n",
    "\n",
    "model.add(RepeatVector(3))\n",
    "# now: model.output_shape == (None, 3, 32)\n",
    "\"\"\"\n",
    "alpha_repeated = Permute([2, 1])(RepeatVector(hidden_dims)(alpha))\n",
    "#shape of RepeatVector(hidden_dims)(alpha) is (hidden_dims, seq_length)\n",
    "#shape of Permute([2, 1])(RepeatVector(hidden_dims)(alpha)) is (seq_length, hidden_dims)\n",
    "c = Multiply()([x, alpha_repeated])\n",
    "c = Lambda(lambda xin: K.sum(xin, axis=1), output_shape=(hidden_dims,))(c)\n",
    "#x = Dropout(0.2)(x)\n",
    "output_text = Dense(num_vocabs, activation=\"softmax\")(c)\n",
    "\n",
    "model = Model(input_text, output_text)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file=\"attention_lstm_model.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=Adam(),\n",
    "             metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "423312/423312 [==============================] - 38s 90us/step - loss: 5.5953 - acc: 0.1539\n",
      "Epoch 2/50\n",
      "423312/423312 [==============================] - 34s 80us/step - loss: 4.4700 - acc: 0.2559\n",
      "Epoch 3/50\n",
      "423312/423312 [==============================] - 34s 80us/step - loss: 4.0348 - acc: 0.2957\n",
      "Epoch 4/50\n",
      "423312/423312 [==============================] - 34s 79us/step - loss: 3.7507 - acc: 0.3211\n",
      "Epoch 5/50\n",
      "423312/423312 [==============================] - 34s 81us/step - loss: 3.5324 - acc: 0.3422\n",
      "Epoch 6/50\n",
      "423312/423312 [==============================] - 33s 78us/step - loss: 3.3491 - acc: 0.3607\n",
      "Epoch 7/50\n",
      "423312/423312 [==============================] - 35s 82us/step - loss: 3.1879 - acc: 0.3784\n",
      "Epoch 8/50\n",
      "423312/423312 [==============================] - 35s 84us/step - loss: 3.0436 - acc: 0.3958\n",
      "Epoch 9/50\n",
      "423312/423312 [==============================] - 36s 85us/step - loss: 2.9106 - acc: 0.4140\n",
      "Epoch 10/50\n",
      "423312/423312 [==============================] - 36s 85us/step - loss: 2.7878 - acc: 0.4312\n",
      "Epoch 11/50\n",
      "423312/423312 [==============================] - 35s 83us/step - loss: 2.6733 - acc: 0.4476\n",
      "Epoch 12/50\n",
      "423312/423312 [==============================] - 36s 84us/step - loss: 2.5665 - acc: 0.4647\n",
      "Epoch 13/50\n",
      "423312/423312 [==============================] - 35s 83us/step - loss: 2.4653 - acc: 0.4816\n",
      "Epoch 14/50\n",
      "423312/423312 [==============================] - 33s 79us/step - loss: 2.3715 - acc: 0.4967\n",
      "Epoch 15/50\n",
      "423312/423312 [==============================] - 32s 75us/step - loss: 2.2824 - acc: 0.5131\n",
      "Epoch 16/50\n",
      "423312/423312 [==============================] - 32s 76us/step - loss: 2.1985 - acc: 0.5277\n",
      "Epoch 17/50\n",
      "423312/423312 [==============================] - 32s 76us/step - loss: 2.1214 - acc: 0.5425\n",
      "Epoch 18/50\n",
      "423312/423312 [==============================] - 32s 76us/step - loss: 2.0454 - acc: 0.5562\n",
      "Epoch 19/50\n",
      "423312/423312 [==============================] - 32s 74us/step - loss: 1.9773 - acc: 0.5697\n",
      "Epoch 20/50\n",
      "423312/423312 [==============================] - 31s 74us/step - loss: 1.9102 - acc: 0.5819\n",
      "Epoch 21/50\n",
      "423312/423312 [==============================] - 31s 74us/step - loss: 1.8496 - acc: 0.5935\n",
      "Epoch 22/50\n",
      "423312/423312 [==============================] - 31s 74us/step - loss: 1.7932 - acc: 0.6047\n",
      "Epoch 23/50\n",
      "423312/423312 [==============================] - 34s 81us/step - loss: 1.7392 - acc: 0.6158\n",
      "Epoch 24/50\n",
      "423312/423312 [==============================] - 35s 83us/step - loss: 1.6891 - acc: 0.6249\n",
      "Epoch 25/50\n",
      "423312/423312 [==============================] - 35s 82us/step - loss: 1.6418 - acc: 0.6346\n",
      "Epoch 26/50\n",
      "423312/423312 [==============================] - 34s 79us/step - loss: 1.5981 - acc: 0.6437\n",
      "Epoch 27/50\n",
      "423312/423312 [==============================] - 34s 79us/step - loss: 1.5575 - acc: 0.6513\n",
      "Epoch 28/50\n",
      "423312/423312 [==============================] - 34s 80us/step - loss: 1.5199 - acc: 0.6588\n",
      "Epoch 29/50\n",
      "423312/423312 [==============================] - 33s 78us/step - loss: 1.4846 - acc: 0.6659\n",
      "Epoch 30/50\n",
      "423312/423312 [==============================] - 31s 74us/step - loss: 1.4500 - acc: 0.6726\n",
      "Epoch 31/50\n",
      "423312/423312 [==============================] - 33s 77us/step - loss: 1.4200 - acc: 0.6785\n",
      "Epoch 32/50\n",
      "423312/423312 [==============================] - 34s 79us/step - loss: 1.3927 - acc: 0.6839\n",
      "Epoch 33/50\n",
      "423312/423312 [==============================] - 34s 79us/step - loss: 1.3638 - acc: 0.6895\n",
      "Epoch 34/50\n",
      "423312/423312 [==============================] - 34s 79us/step - loss: 1.3389 - acc: 0.6944\n",
      "Epoch 35/50\n",
      "423312/423312 [==============================] - 33s 79us/step - loss: 1.3142 - acc: 0.6996\n",
      "Epoch 36/50\n",
      "423312/423312 [==============================] - 34s 80us/step - loss: 1.2932 - acc: 0.7034\n",
      "Epoch 37/50\n",
      "423312/423312 [==============================] - 34s 81us/step - loss: 1.2720 - acc: 0.7074\n",
      "Epoch 38/50\n",
      "423312/423312 [==============================] - 35s 82us/step - loss: 1.2530 - acc: 0.7097\n",
      "Epoch 39/50\n",
      "423312/423312 [==============================] - 35s 82us/step - loss: 1.2356 - acc: 0.7135\n",
      "Epoch 40/50\n",
      "423312/423312 [==============================] - 34s 81us/step - loss: 1.2189 - acc: 0.7173\n",
      "Epoch 41/50\n",
      "423312/423312 [==============================] - 34s 80us/step - loss: 1.2032 - acc: 0.7197\n",
      "Epoch 42/50\n",
      "423312/423312 [==============================] - 34s 80us/step - loss: 1.1867 - acc: 0.7230\n",
      "Epoch 43/50\n",
      "423312/423312 [==============================] - 34s 80us/step - loss: 1.1760 - acc: 0.7239\n",
      "Epoch 44/50\n",
      "423312/423312 [==============================] - 34s 80us/step - loss: 1.1610 - acc: 0.7274\n",
      "Epoch 45/50\n",
      "423312/423312 [==============================] - 34s 80us/step - loss: 1.1519 - acc: 0.7284\n",
      "Epoch 46/50\n",
      "423312/423312 [==============================] - 34s 81us/step - loss: 1.1400 - acc: 0.7304\n",
      "Epoch 47/50\n",
      "423312/423312 [==============================] - 35s 82us/step - loss: 1.1299 - acc: 0.7325\n",
      "Epoch 48/50\n",
      "423312/423312 [==============================] - 35s 82us/step - loss: 1.1218 - acc: 0.7327\n",
      "Epoch 49/50\n",
      "423312/423312 [==============================] - 34s 80us/step - loss: 1.1115 - acc: 0.7359\n",
      "Epoch 50/50\n",
      "423312/423312 [==============================] - 34s 81us/step - loss: 1.1031 - acc: 0.7370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fff7ef543c8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on \"temperature\" to alter a distribution\n",
    "A parameter called **\"temperature(softmax temperature)\"** is used to alter the original distribution below.\n",
    "<br>\n",
    "<br>\n",
    "$$\\exp(\\frac{\\log(original\\ distribution)}{temperature})$$\n",
    "$$= exp(\\log(original\\ distribution)) \\times exp(-temperature)$$\n",
    "$$= original\\ distribution \\times \\exp(-temperature)$$\n",
    "$$where\\ 0 \\neq temperature$$\n",
    "$$(use\\ argmax(original distribution)\\ where\\ temperature = 0)$$\n",
    "<br>\n",
    "That is, by multiplying original distribution by a number which is less than 1(equivalent to say temperature > 0), difference between high probabilities of some indexes and low probabilities of other indexes become small, so that the original distribution will have larger entropy(getting closer to uniform distribution, which is more unpredictable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alter_dist_and_pick_one(orig_dist, temp):\n",
    "    if temp == 0:\n",
    "        return np.argmax(orig_dist)\n",
    "    else:\n",
    "        dist = np.log(orig_dist + 1e-7) / temp\n",
    "        dist = np.exp(dist)\n",
    "        normalized_dist = dist / np.sum(dist)\n",
    "        choices = range(len(normalized_dist))\n",
    "        return np.random.choice(choices, p=normalized_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, max_len, temp=0.1):\n",
    "    generated_text = \"【Sports Watch】日本を代表する打者である松井秀喜氏は\"\n",
    "    for i in range(max_len):\n",
    "        int_tokens = tokenizer.EncodeAsIds(generated_text)\n",
    "        #update int_tokens to predict next token\n",
    "        int_tokens = int_tokens[-seq_length:]\n",
    "        int_tokens = np.reshape(int_tokens, (1,seq_length))\n",
    "        pred_dist = model.predict(int_tokens, verbose=0)[0]\n",
    "        \n",
    "        pred_index = alter_dist_and_pick_one(pred_dist, temp)\n",
    "        generated_text += tokenizer.IdToPiece(int(pred_index))\n",
    "        \n",
    "        if generated_text[-4:] == \"</s>\" or generated_text[-3:] == \"▁▁▁\":\n",
    "            break\n",
    "        \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【Sports Watch】日本を代表する打者である松井秀喜氏は、今回のブログを更新、紗栄子に急接近する「新たな男」の存在が困る浅尾美和。今も大きな話題となった。▁▁▁\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(model, 1000, 0.3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
