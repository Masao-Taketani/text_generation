{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "626594"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "CORPUS_PATH = \"livedoor_data/text/sports-watch/*.txt\"\n",
    "text_corpus = glob(CORPUS_PATH)\n",
    "start_token = \"<s>\"\n",
    "end_token = \"</s>\"\n",
    "entire_text = \"\"\n",
    "\n",
    "for filepath in text_corpus:\n",
    "    if filepath == \"livedoor_data/text/sports-watch/LICENSE.txt\":\n",
    "        continue\n",
    "    else:\n",
    "        tmp_text = \"\" + start_token\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            #skip first 2 rows for each document\n",
    "            #1st row:referred URL, 2nd row:the article-written date\n",
    "            for i in range(2):\n",
    "                next(f)\n",
    "            tmp_text += f.read()\n",
    "            entire_text += tmp_text\n",
    "            entire_text += end_token\n",
    "        \n",
    "len(entire_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '<', 's', '>', '【', 'S', 'p', 'or', 't', 's', '▁W', 'at', 'ch', '】', '秋', '山', '成', '勲', '、', 'メール', 'で', '吉田', 'に', '対戦', '迫', 'った', '!', '?', '▁', '今', '月', '8', '日', '、', '都', '内', 'ホテル', 'では', '、', '総合', '格闘', '家', '・', '吉田', '秀', '彦', 'の', '引退', '試合', '興行', '「', 'A', 'ST', 'RA', '」', 'の', '開催', 'が', '発表された', '。', '▁', 'バル', 'セ', 'ロ', 'ナ', '五', '輪', '柔道', '金', 'メ', 'ダ', 'リスト', 'としての', '実', '績', 'を', '引', 'っ', 'さ', 'げ', '、', '2002', '年に', 'プロ', '総合', '格闘', '家', 'に転', '向', '。', '以後', '、', '数', '々', 'の', '死', '闘', 'を', '繰り', '広']\n",
      "length of str_tokens: 423332\n",
      "[6, 2003, 160, 1954, 6050, 130, 334, 531, 268, 160, 1677, 1055, 945, 6051, 1341, 63, 391, 5563, 3, 4893, 14, 4366, 10, 2789, 2840, 105, 947, 2607, 6, 702, 19, 62, 30, 3, 949, 155, 2839, 36, 3, 1498, 5311, 122, 11, 4366, 2046, 2299, 4, 1583, 569, 4972, 20, 141, 1997, 2774, 18, 4, 814, 9, 2508, 5, 6, 1077, 266, 83, 90, 810, 1216, 5470, 220, 188, 246, 1610, 1764, 291, 4879, 8, 1301, 360, 202, 862, 3, 1386, 72, 524, 1498, 5311, 122, 3469, 613, 5, 1816, 3, 158, 502, 4, 519, 2443, 8, 4461, 466]\n",
      "length of int_tokens: 423332\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "tokenizer = spm.SentencePieceProcessor()\n",
    "tokenizer.Load(\"wiki_data/wikiextractor/spm.model\")\n",
    "\n",
    "str_tokens = tokenizer.EncodeAsPieces(entire_text)\n",
    "print(str_tokens[:100])\n",
    "print(\"length of str_tokens:\", len(str_tokens))\n",
    "\n",
    "int_tokens = []\n",
    "for token in str_tokens:\n",
    "    int_tokens.append(tokenizer.piece_to_id(token))\n",
    "    \n",
    "print(int_tokens[:100])\n",
    "print(\"length of int_tokens:\", len(int_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((423312, 20), (423312, 8000))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "input_texts, target_texts = [], []\n",
    "seq_length = 20\n",
    "num_vocabs = 8000\n",
    "\n",
    "for i in range(0, len(int_tokens) - seq_length, 1):\n",
    "    input_texts.append(int_tokens[i: i + seq_length])\n",
    "    target_texts.append(int_tokens[i + seq_length])\n",
    "    \n",
    "target_texts_one_hot = to_categorical(target_texts, num_classes=num_vocabs)\n",
    "X = np.array(input_texts)\n",
    "y = np.array(target_texts_one_hot)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 300)    2400000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)        (None, 256)          571392      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           cu_dnnlstm_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1)            0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 256, 1)       0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 1, 256)       0           repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 1, 256)       0           dropout_1[0][0]                  \n",
      "                                                                 permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 256)          0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 8000)         2056000     lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 5,027,649\n",
      "Trainable params: 5,027,649\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, CuDNNLSTM, Dropout, Dense, Reshape, Activation, Permute, Multiply, Lambda, RepeatVector\n",
    "import keras.backend as K\n",
    "\n",
    "embed_dims = 300\n",
    "hidden_dims = 256\n",
    "\n",
    "input_text = Input((None,))\n",
    "x = Embedding(num_vocabs, embed_dims)(input_text)\n",
    "x = CuDNNLSTM(hidden_dims)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "#shape of x is (seq_length, hidden_dims)\n",
    "##attention part\n",
    "e = Dense(1, activation=\"tanh\")(x)\n",
    "e = Reshape([-1])(e)\n",
    "alpha = Activation(\"softmax\")(e)\n",
    "\"\"\"\n",
    "Explanation of Permute\n",
    "model = Sequential()\n",
    "model.add(Permute((2, 1), input_shape=(10, 64)))\n",
    "# now: model.output_shape == (None, 64, 10)\n",
    "# note: `None` is the batch dimension\n",
    "\n",
    "Explanation of RepeatVector\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=32))\n",
    "# now: model.output_shape == (None, 32)\n",
    "# note: `None` is the batch dimension\n",
    "\n",
    "model.add(RepeatVector(3))\n",
    "# now: model.output_shape == (None, 3, 32)\n",
    "\"\"\"\n",
    "alpha_repeated = Permute([2, 1])(RepeatVector(hidden_dims)(alpha))\n",
    "#shape of RepeatVector(hidden_dims)(alpha) is (hidden_dims, seq_length)\n",
    "#shape of Permute([2, 1])(RepeatVector(hidden_dims)(alpha)) is (seq_length, hidden_dims)\n",
    "c = Multiply()([x, alpha_repeated])\n",
    "c = Lambda(lambda xin: K.sum(xin, axis=1), output_shape=(hidden_dims,))(c)\n",
    "\n",
    "output_text = Dense(num_vocabs, activation=\"softmax\")(c)\n",
    "\n",
    "model = Model(input_text, output_text)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file=\"attention_lstm_model.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=Adam(),\n",
    "             metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "423312/423312 [==============================] - 36s 85us/step - loss: 5.6992 - acc: 0.1445\n",
      "Epoch 2/50\n",
      "423312/423312 [==============================] - 31s 74us/step - loss: 4.6025 - acc: 0.2432\n",
      "Epoch 3/50\n",
      "423312/423312 [==============================] - 32s 75us/step - loss: 4.1829 - acc: 0.2811\n",
      "Epoch 4/50\n",
      "423312/423312 [==============================] - 32s 75us/step - loss: 3.9198 - acc: 0.3047\n",
      "Epoch 5/50\n",
      "423312/423312 [==============================] - 32s 75us/step - loss: 3.7218 - acc: 0.3217\n",
      "Epoch 6/50\n",
      "423312/423312 [==============================] - 32s 75us/step - loss: 3.5612 - acc: 0.3364\n",
      "Epoch 7/50\n",
      "423312/423312 [==============================] - 32s 75us/step - loss: 3.4225 - acc: 0.3506\n",
      "Epoch 8/50\n",
      "423312/423312 [==============================] - 32s 76us/step - loss: 3.2997 - acc: 0.3632\n",
      "Epoch 9/50\n",
      "423312/423312 [==============================] - 32s 76us/step - loss: 3.1893 - acc: 0.3754\n",
      "Epoch 10/50\n",
      "423312/423312 [==============================] - 32s 77us/step - loss: 3.0896 - acc: 0.3859\n",
      "Epoch 11/50\n",
      "423312/423312 [==============================] - 32s 76us/step - loss: 2.9976 - acc: 0.3973\n",
      "Epoch 12/50\n",
      "423312/423312 [==============================] - 33s 77us/step - loss: 2.9106 - acc: 0.4087\n",
      "Epoch 13/50\n",
      "423312/423312 [==============================] - 33s 77us/step - loss: 2.8321 - acc: 0.4195\n",
      "Epoch 14/50\n",
      "423312/423312 [==============================] - 33s 77us/step - loss: 2.7600 - acc: 0.4293\n",
      "Epoch 15/50\n",
      "423312/423312 [==============================] - 33s 77us/step - loss: 2.6915 - acc: 0.4378\n",
      "Epoch 16/50\n",
      "423312/423312 [==============================] - 33s 77us/step - loss: 2.6296 - acc: 0.4474\n",
      "Epoch 17/50\n",
      "423312/423312 [==============================] - 32s 77us/step - loss: 2.5686 - acc: 0.4558\n",
      "Epoch 18/50\n",
      "423312/423312 [==============================] - 33s 77us/step - loss: 2.5142 - acc: 0.4644\n",
      "Epoch 19/50\n",
      "423312/423312 [==============================] - 33s 79us/step - loss: 2.4659 - acc: 0.4708\n",
      "Epoch 20/50\n",
      "423312/423312 [==============================] - ETA: 0s - loss: 2.4166 - acc: 0.477 - 33s 78us/step - loss: 2.4168 - acc: 0.4777\n",
      "Epoch 21/50\n",
      "423312/423312 [==============================] - 33s 79us/step - loss: 2.3746 - acc: 0.4847\n",
      "Epoch 22/50\n",
      "423312/423312 [==============================] - 34s 80us/step - loss: 2.3349 - acc: 0.4907\n",
      "Epoch 23/50\n",
      "423312/423312 [==============================] - 34s 80us/step - loss: 2.2960 - acc: 0.4964\n",
      "Epoch 24/50\n",
      "423312/423312 [==============================] - 33s 78us/step - loss: 2.2622 - acc: 0.5010\n",
      "Epoch 25/50\n",
      "423312/423312 [==============================] - 33s 77us/step - loss: 2.2313 - acc: 0.5065\n",
      "Epoch 26/50\n",
      "423312/423312 [==============================] - 32s 76us/step - loss: 2.2058 - acc: 0.5095\n",
      "Epoch 27/50\n",
      "423312/423312 [==============================] - 33s 77us/step - loss: 2.1742 - acc: 0.5147\n",
      "Epoch 28/50\n",
      "423312/423312 [==============================] - 33s 77us/step - loss: 2.1518 - acc: 0.5182\n",
      "Epoch 29/50\n",
      "423312/423312 [==============================] - 32s 76us/step - loss: 2.1317 - acc: 0.5210\n",
      "Epoch 30/50\n",
      "423312/423312 [==============================] - 32s 77us/step - loss: 2.1103 - acc: 0.5251\n",
      "Epoch 31/50\n",
      "423312/423312 [==============================] - 33s 78us/step - loss: 2.0910 - acc: 0.5270\n",
      "Epoch 32/50\n",
      "423312/423312 [==============================] - 33s 78us/step - loss: 2.0710 - acc: 0.5303\n",
      "Epoch 33/50\n",
      "423312/423312 [==============================] - 33s 78us/step - loss: 2.0567 - acc: 0.5332\n",
      "Epoch 34/50\n",
      "423312/423312 [==============================] - 33s 78us/step - loss: 2.0425 - acc: 0.5343\n",
      "Epoch 35/50\n",
      "423312/423312 [==============================] - 33s 77us/step - loss: 2.0274 - acc: 0.5368\n",
      "Epoch 36/50\n",
      "423312/423312 [==============================] - 33s 78us/step - loss: 2.0190 - acc: 0.5375\n",
      "Epoch 37/50\n",
      "423312/423312 [==============================] - 33s 78us/step - loss: 2.0035 - acc: 0.5405\n",
      "Epoch 38/50\n",
      "423312/423312 [==============================] - 33s 77us/step - loss: 1.9907 - acc: 0.5417\n",
      "Epoch 39/50\n",
      "423312/423312 [==============================] - 33s 77us/step - loss: 1.9867 - acc: 0.5420\n",
      "Epoch 40/50\n",
      "423312/423312 [==============================] - 33s 78us/step - loss: 1.9741 - acc: 0.5435\n",
      "Epoch 41/50\n",
      "423312/423312 [==============================] - 33s 79us/step - loss: 1.9671 - acc: 0.5452\n",
      "Epoch 42/50\n",
      "423312/423312 [==============================] - 34s 80us/step - loss: 1.9577 - acc: 0.5467\n",
      "Epoch 43/50\n",
      "423312/423312 [==============================] - 34s 80us/step - loss: 1.9526 - acc: 0.5476\n",
      "Epoch 44/50\n",
      "423312/423312 [==============================] - 33s 79us/step - loss: 1.9469 - acc: 0.5483\n",
      "Epoch 45/50\n",
      "423312/423312 [==============================] - 33s 78us/step - loss: 1.9440 - acc: 0.5484\n",
      "Epoch 46/50\n",
      "423312/423312 [==============================] - 33s 79us/step - loss: 1.9376 - acc: 0.5495\n",
      "Epoch 47/50\n",
      "423312/423312 [==============================] - 34s 79us/step - loss: 1.9275 - acc: 0.5504\n",
      "Epoch 48/50\n",
      "423312/423312 [==============================] - 33s 77us/step - loss: 1.9258 - acc: 0.5505\n",
      "Epoch 49/50\n",
      "423312/423312 [==============================] - 34s 80us/step - loss: 1.9242 - acc: 0.5504\n",
      "Epoch 50/50\n",
      "423312/423312 [==============================] - 33s 78us/step - loss: 1.9177 - acc: 0.5518\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff8ed95cdd8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on \"temperature\" to alter a distribution\n",
    "A parameter called **\"temperature(softmax temperature)\"** is used to alter the original distribution below.\n",
    "<br>\n",
    "<br>\n",
    "$$\\exp(\\frac{\\log(original\\ distribution)}{temperature})$$\n",
    "$$= exp(\\log(original\\ distribution)) \\times exp(-temperature)$$\n",
    "$$= original\\ distribution \\times \\exp(-temperature)$$\n",
    "$$where\\ 0 \\neq temperature$$\n",
    "$$(use\\ argmax(original distribution)\\ where\\ temperature = 0)$$\n",
    "<br>\n",
    "That is, by multiplying original distribution by a number which is less than 1(equivalent to say temperature > 0), difference between high probabilities of some indexes and low probabilities of other indexes become small, so that the original distribution will have larger entropy(getting closer to uniform distribution, which is more unpredictable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alter_dist_and_pick_one(orig_dist, temp):\n",
    "    if temp == 0:\n",
    "        return np.argmax(orig_dist)\n",
    "    else:\n",
    "        dist = np.log(orig_dist + 1e-7) / temp\n",
    "        dist = np.exp(dist)\n",
    "        normalized_dist = dist / np.sum(dist)\n",
    "        choices = range(len(normalized_dist))\n",
    "        return np.random.choice(choices, p=normalized_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, max_len, temp=0.1):\n",
    "    generated_text = \"【Sports Watch】日本を代表する打者である松井秀喜氏は\"\n",
    "    for i in range(max_len):\n",
    "        int_tokens = tokenizer.EncodeAsIds(generated_text)\n",
    "        #update int_tokens to predict next token\n",
    "        int_tokens = int_tokens[-seq_length:]\n",
    "        int_tokens = np.reshape(int_tokens, (1,seq_length))\n",
    "        pred_dist = model.predict(int_tokens, verbose=0)[0]\n",
    "        \n",
    "        pred_index = alter_dist_and_pick_one(pred_dist, temp)\n",
    "        generated_text += tokenizer.IdToPiece(int(pred_index))\n",
    "        \n",
    "        if generated_text[-4:] == \"</s>\" or generated_text[-3:] == \"▁▁▁\":\n",
    "            break\n",
    "        \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【Sports Watch】日本を代表する打者である松井秀喜氏は「僕は、この4点を生みましたね。(笑)」とツッコミを話した。▁▁▁\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(model, 1000, 0.3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
