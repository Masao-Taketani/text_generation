{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "626594"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "CORPUS_PATH = \"livedoor_data/text/sports-watch/*.txt\"\n",
    "text_corpus = glob(CORPUS_PATH)\n",
    "start_token = \"<s>\"\n",
    "end_token = \"</s>\"\n",
    "entire_text = \"\"\n",
    "\n",
    "for filepath in text_corpus:\n",
    "    if filepath == \"livedoor_data/text/sports-watch/LICENSE.txt\":\n",
    "        continue\n",
    "    else:\n",
    "        tmp_text = \"\" + start_token\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            #skip first 2 rows for each document\n",
    "            #1st row:referred URL, 2nd row:the article-written date\n",
    "            for i in range(2):\n",
    "                next(f)\n",
    "            tmp_text += f.read()\n",
    "            entire_text += tmp_text\n",
    "            entire_text += end_token\n",
    "        \n",
    "len(entire_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '<', 's', '>', '【', 'S', 'p', 'or', 't', 's', '▁W', 'at', 'ch', '】', '秋', '山', '成', '勲', '、', 'メール', 'で', '吉田', 'に', '対戦', '迫', 'った', '!', '?', '▁', '今', '月', '8', '日', '、', '都', '内', 'ホテル', 'では', '、', '総合', '格闘', '家', '・', '吉田', '秀', '彦', 'の', '引退', '試合', '興行', '「', 'A', 'ST', 'RA', '」', 'の', '開催', 'が', '発表された', '。', '▁', 'バル', 'セ', 'ロ', 'ナ', '五', '輪', '柔道', '金', 'メ', 'ダ', 'リスト', 'としての', '実', '績', 'を', '引', 'っ', 'さ', 'げ', '、', '2002', '年に', 'プロ', '総合', '格闘', '家', 'に転', '向', '。', '以後', '、', '数', '々', 'の', '死', '闘', 'を', '繰り', '広']\n",
      "length of str_tokens: 423332\n",
      "[6, 2003, 160, 1954, 6050, 130, 334, 531, 268, 160, 1677, 1055, 945, 6051, 1341, 63, 391, 5563, 3, 4893, 14, 4366, 10, 2789, 2840, 105, 947, 2607, 6, 702, 19, 62, 30, 3, 949, 155, 2839, 36, 3, 1498, 5311, 122, 11, 4366, 2046, 2299, 4, 1583, 569, 4972, 20, 141, 1997, 2774, 18, 4, 814, 9, 2508, 5, 6, 1077, 266, 83, 90, 810, 1216, 5470, 220, 188, 246, 1610, 1764, 291, 4879, 8, 1301, 360, 202, 862, 3, 1386, 72, 524, 1498, 5311, 122, 3469, 613, 5, 1816, 3, 158, 502, 4, 519, 2443, 8, 4461, 466]\n",
      "length of int_tokens: 423332\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "tokenizer = spm.SentencePieceProcessor()\n",
    "tokenizer.Load(\"wiki_data/wikiextractor/spm.model\")\n",
    "\n",
    "str_tokens = tokenizer.EncodeAsPieces(entire_text)\n",
    "print(str_tokens[:100])\n",
    "print(\"length of str_tokens:\", len(str_tokens))\n",
    "\n",
    "int_tokens = []\n",
    "for token in str_tokens:\n",
    "    int_tokens.append(tokenizer.piece_to_id(token))\n",
    "    \n",
    "print(int_tokens[:100])\n",
    "print(\"length of int_tokens:\", len(int_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((423312, 20), (423312, 8000))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "input_texts, target_texts = [], []\n",
    "seq_length = 20\n",
    "num_vocabs = 8000\n",
    "\n",
    "for i in range(0, len(int_tokens) - seq_length, 1):\n",
    "    input_texts.append(int_tokens[i: i + seq_length])\n",
    "    target_texts.append(int_tokens[i + seq_length])\n",
    "    \n",
    "target_texts_one_hot = to_categorical(target_texts, num_classes=num_vocabs)\n",
    "X = np.array(input_texts)\n",
    "y = np.array(target_texts_one_hot)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, None, 300)         2400000   \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 256)               571392    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8000)              2056000   \n",
      "=================================================================\n",
      "Total params: 5,027,392\n",
      "Trainable params: 5,027,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, CuDNNLSTM, Dropout, Dense\n",
    "\n",
    "hidden_dims = 256\n",
    "embed_dims = 300\n",
    "\n",
    "input_text = Input((None,))\n",
    "x = Embedding(num_vocabs, embed_dims)(input_text)\n",
    "x = CuDNNLSTM(hidden_dims)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "output_text = Dense(num_vocabs, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(input_text, output_text)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=Adam(),\n",
    "             metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "423312/423312 [==============================] - 35s 84us/step - loss: 5.6966 - acc: 0.1446\n",
      "Epoch 2/50\n",
      "423312/423312 [==============================] - 32s 76us/step - loss: 4.6223 - acc: 0.2421\n",
      "Epoch 3/50\n",
      "423312/423312 [==============================] - 32s 76us/step - loss: 4.2050 - acc: 0.2795\n",
      "Epoch 4/50\n",
      "423312/423312 [==============================] - 32s 75us/step - loss: 3.9425 - acc: 0.3015\n",
      "Epoch 5/50\n",
      "423312/423312 [==============================] - 32s 76us/step - loss: 3.7464 - acc: 0.3182\n",
      "Epoch 6/50\n",
      "423312/423312 [==============================] - 32s 77us/step - loss: 3.5883 - acc: 0.3332\n",
      "Epoch 7/50\n",
      "423312/423312 [==============================] - 33s 77us/step - loss: 3.4521 - acc: 0.3464\n",
      "Epoch 8/50\n",
      "423312/423312 [==============================] - 32s 76us/step - loss: 3.3311 - acc: 0.3585\n",
      "Epoch 9/50\n",
      "423312/423312 [==============================] - 32s 76us/step - loss: 3.2218 - acc: 0.3704\n",
      "Epoch 10/50\n",
      "423312/423312 [==============================] - 32s 76us/step - loss: 3.1221 - acc: 0.3816\n",
      "Epoch 11/50\n",
      "423312/423312 [==============================] - 31s 74us/step - loss: 3.0317 - acc: 0.3919\n",
      "Epoch 12/50\n",
      "423312/423312 [==============================] - 31s 74us/step - loss: 2.9496 - acc: 0.4035\n",
      "Epoch 13/50\n",
      "423312/423312 [==============================] - 32s 75us/step - loss: 2.8722 - acc: 0.4125\n",
      "Epoch 14/50\n",
      "423312/423312 [==============================] - 32s 76us/step - loss: 2.7990 - acc: 0.4220\n",
      "Epoch 15/50\n",
      "423312/423312 [==============================] - 32s 76us/step - loss: 2.7298 - acc: 0.4316\n",
      "Epoch 16/50\n",
      "423312/423312 [==============================] - 32s 76us/step - loss: 2.6663 - acc: 0.4410\n",
      "Epoch 17/50\n",
      "423312/423312 [==============================] - 32s 75us/step - loss: 2.6074 - acc: 0.4489\n",
      "Epoch 18/50\n",
      "423312/423312 [==============================] - 29s 69us/step - loss: 2.5529 - acc: 0.4570\n",
      "Epoch 19/50\n",
      "423312/423312 [==============================] - 29s 68us/step - loss: 2.5020 - acc: 0.4640\n",
      "Epoch 20/50\n",
      "423312/423312 [==============================] - 29s 69us/step - loss: 2.4564 - acc: 0.4708\n",
      "Epoch 21/50\n",
      "423312/423312 [==============================] - 29s 69us/step - loss: 2.4097 - acc: 0.4775\n",
      "Epoch 22/50\n",
      "423312/423312 [==============================] - 29s 69us/step - loss: 2.3714 - acc: 0.4830\n",
      "Epoch 23/50\n",
      "423312/423312 [==============================] - 29s 69us/step - loss: 2.3322 - acc: 0.4899\n",
      "Epoch 24/50\n",
      "423312/423312 [==============================] - 29s 69us/step - loss: 2.2939 - acc: 0.4961\n",
      "Epoch 25/50\n",
      "423312/423312 [==============================] - 29s 68us/step - loss: 2.2659 - acc: 0.4992\n",
      "Epoch 26/50\n",
      "423312/423312 [==============================] - 29s 69us/step - loss: 2.2349 - acc: 0.5041\n",
      "Epoch 27/50\n",
      "423312/423312 [==============================] - 29s 68us/step - loss: 2.2069 - acc: 0.5094\n",
      "Epoch 28/50\n",
      "423312/423312 [==============================] - 29s 68us/step - loss: 2.1809 - acc: 0.5131\n",
      "Epoch 29/50\n",
      "423312/423312 [==============================] - 29s 68us/step - loss: 2.1596 - acc: 0.5161\n",
      "Epoch 30/50\n",
      "423312/423312 [==============================] - 29s 69us/step - loss: 2.1349 - acc: 0.5205\n",
      "Epoch 31/50\n",
      "423312/423312 [==============================] - 29s 70us/step - loss: 2.1128 - acc: 0.5232\n",
      "Epoch 32/50\n",
      "423312/423312 [==============================] - 30s 70us/step - loss: 2.0960 - acc: 0.5262\n",
      "Epoch 33/50\n",
      "423312/423312 [==============================] - 30s 70us/step - loss: 2.0802 - acc: 0.5279\n",
      "Epoch 34/50\n",
      "423312/423312 [==============================] - 29s 69us/step - loss: 2.0590 - acc: 0.5313\n",
      "Epoch 35/50\n",
      "423312/423312 [==============================] - 29s 69us/step - loss: 2.0457 - acc: 0.5340\n",
      "Epoch 36/50\n",
      "423312/423312 [==============================] - 29s 68us/step - loss: 2.0321 - acc: 0.5355\n",
      "Epoch 37/50\n",
      "423312/423312 [==============================] - 29s 68us/step - loss: 2.0175 - acc: 0.5379\n",
      "Epoch 38/50\n",
      "423312/423312 [==============================] - 29s 68us/step - loss: 2.0093 - acc: 0.5397\n",
      "Epoch 39/50\n",
      "423312/423312 [==============================] - 28s 67us/step - loss: 1.9984 - acc: 0.5402\n",
      "Epoch 40/50\n",
      "423312/423312 [==============================] - 28s 67us/step - loss: 1.9910 - acc: 0.5415\n",
      "Epoch 41/50\n",
      "423312/423312 [==============================] - 29s 69us/step - loss: 1.9808 - acc: 0.5431\n",
      "Epoch 42/50\n",
      "423312/423312 [==============================] - 29s 68us/step - loss: 1.9714 - acc: 0.5441\n",
      "Epoch 43/50\n",
      "423312/423312 [==============================] - 29s 68us/step - loss: 1.9642 - acc: 0.5456\n",
      "Epoch 44/50\n",
      "423312/423312 [==============================] - 29s 68us/step - loss: 1.9583 - acc: 0.5457\n",
      "Epoch 45/50\n",
      "423312/423312 [==============================] - 29s 69us/step - loss: 1.9504 - acc: 0.5476\n",
      "Epoch 46/50\n",
      "423312/423312 [==============================] - 29s 68us/step - loss: 1.9437 - acc: 0.5488\n",
      "Epoch 47/50\n",
      "423312/423312 [==============================] - 29s 69us/step - loss: 1.9408 - acc: 0.5486\n",
      "Epoch 48/50\n",
      "423312/423312 [==============================] - 29s 69us/step - loss: 1.9321 - acc: 0.5495\n",
      "Epoch 49/50\n",
      "423312/423312 [==============================] - 29s 69us/step - loss: 1.9291 - acc: 0.5502\n",
      "Epoch 50/50\n",
      "423312/423312 [==============================] - 29s 69us/step - loss: 1.9226 - acc: 0.5507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fff284bf7b8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## note on \"temperature\" to alter a distribution\n",
    "A parameter called **\"temperature(softmax temperature)\"** is used to alter the original distribution below.\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "    \\exp(\\frac{\\log(original\\ distribution)}{temperature})\\\\\n",
    "    = exp(\\log(original\\ distribution)) \\times exp(-temperature)\\\\\n",
    "    = original\\ distribution \\times \\exp(-temperature)\\\\\n",
    "    where\\ 0 \\neq temperature\n",
    "$$\n",
    "<br>\n",
    "That is, by multiplying original distribution by a number which is less than 1, difference between high probabilities of some indexes and low probabilities of other indexes become small, so that the original distribution will have larger entropy(getting closer to uniform distribution, which is more unpredictable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alter_dist_and_pick_one(orig_dist, temp):\n",
    "    dist = np.log(orig_dist + 1e-7) / temp\n",
    "    dist = np.exp(dist)\n",
    "    normalized_dist = dist / np.sum(dist)\n",
    "    choices = range(len(normalized_dist))\n",
    "    return np.random.choice(choices, p=normalized_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, max_len, temp=0.1):\n",
    "    generated_text = \"【Sports Watch】日本を代表する打者である松井秀喜氏は\"\n",
    "    for i in range(max_len):\n",
    "        int_tokens = tokenizer.EncodeAsIds(generated_text)\n",
    "        #update int_tokens to predict next token\n",
    "        int_tokens = int_tokens[-seq_length:]\n",
    "        int_tokens = np.reshape(int_tokens, (1,seq_length))\n",
    "        pred_dist = model.predict(int_tokens, verbose=0)[0]\n",
    "        \n",
    "        pred_index = alter_dist_and_pick_one(pred_dist, temp)\n",
    "        generated_text += tokenizer.IdToPiece(int(pred_index))\n",
    "        \n",
    "        if generated_text[-4:] == \"</s>\" or generated_text[-3:] == \"▁▁▁\":\n",
    "            break\n",
    "        \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【Sports Watch】日本を代表する打者である松井秀喜氏は、「私の場合は、長友がクロスから上がるっていうのは、すごいあったんですよ」と語り、また、チームのエースを「出したいです」と語る。▁▁また、浅田は「今シーズンは21年間、フィジカル的なことをやって、今台のままじゃあっていて、今年はやってますよね。(カズダンスの)タッチにいかない。▁そういう意味では、そういう意味では、自分がやってるから、そういうのは、自分自身で一番最初は出たら、ホッとしました。一回攻撃してくれなかった」と語った。▁▁▁\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(model, 1000, 0.3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
