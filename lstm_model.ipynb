{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "622094"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "CORPUS_PATH = \"livedoor_data/text/sports-watch/*.txt\"\n",
    "text_corpus = glob(CORPUS_PATH)\n",
    "start_token = \"<\"\n",
    "end_token = \">\"\n",
    "entire_text = \"\"\n",
    "\n",
    "for filepath in text_corpus:\n",
    "    if filepath == \"livedoor_data/text/sports-watch/LICENSE.txt\":\n",
    "        continue\n",
    "    else:\n",
    "        tmp_text = start_token\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            #skip first 2 rows for each document\n",
    "            #1st row:referred URL, 2nd row:the article-written date\n",
    "            for i in range(2):\n",
    "                next(f)\n",
    "            tmp_text += f.read()\n",
    "            entire_text += tmp_text\n",
    "            entire_text += end_token\n",
    "        \n",
    "len(entire_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', '【', 'S', 'p', 'or', 't', 's', '▁W', 'at', 'ch', '】', '秋', '山', '成', '勲', '、', 'メール', 'で', '吉田', 'に', '対戦', '迫', 'った', '!', '?', '▁', '今', '月', '8', '日', '、', '都', '内', 'ホテル', 'では', '、', '総合', '格闘', '家', '・', '吉田', '秀', '彦', 'の', '引退', '試合', '興行', '「', 'A', 'ST', 'RA', '」', 'の', '開催', 'が', '発表された', '。', '▁', 'バル', 'セ', 'ロ', 'ナ', '五', '輪', '柔道', '金', 'メ', 'ダ', 'リスト', 'としての', '実', '績', 'を', '引', 'っ', 'さ', 'げ', '、', '2002', '年に', 'プロ', '総合', '格闘', '家', 'に転', '向', '。', '以後', '、', '数', '々', 'の', '死', '闘', 'を', '繰り', '広', 'げ', 'てきた', '吉田']\n",
      "length of str_tokens: 418833\n",
      "[2003, 6050, 130, 334, 531, 268, 160, 1677, 1055, 945, 6051, 1341, 63, 391, 5563, 3, 4893, 14, 4366, 10, 2789, 2840, 105, 947, 2607, 6, 702, 19, 62, 30, 3, 949, 155, 2839, 36, 3, 1498, 5311, 122, 11, 4366, 2046, 2299, 4, 1583, 569, 4972, 20, 141, 1997, 2774, 18, 4, 814, 9, 2508, 5, 6, 1077, 266, 83, 90, 810, 1216, 5470, 220, 188, 246, 1610, 1764, 291, 4879, 8, 1301, 360, 202, 862, 3, 1386, 72, 524, 1498, 5311, 122, 3469, 613, 5, 1816, 3, 158, 502, 4, 519, 2443, 8, 4461, 466, 862, 1787, 4366]\n",
      "length of int_tokens: 418833\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "tokenizer = spm.SentencePieceProcessor()\n",
    "tokenizer.Load(\"wiki_data/wikiextractor/spm.model\")\n",
    "\n",
    "str_tokens = tokenizer.EncodeAsPieces(entire_text)[1:]\n",
    "print(str_tokens[:100])\n",
    "print(\"length of str_tokens:\", len(str_tokens))\n",
    "\n",
    "int_tokens = []\n",
    "for token in str_tokens:\n",
    "    int_tokens.append(tokenizer.piece_to_id(token))\n",
    "    \n",
    "print(int_tokens[:100])\n",
    "print(\"length of int_tokens:\", len(int_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((418813, 20), (418813, 8000))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "input_texts, target_texts = [], []\n",
    "seq_length = 20\n",
    "num_vocabs = 8000\n",
    "\n",
    "for i in range(0, len(int_tokens) - seq_length, 1):\n",
    "    input_texts.append(int_tokens[i: i + seq_length])\n",
    "    target_texts.append(int_tokens[i + seq_length])\n",
    "    \n",
    "target_texts_one_hot = to_categorical(target_texts, num_classes=num_vocabs)\n",
    "X = np.array(input_texts)\n",
    "y = np.array(target_texts_one_hot)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, None, 300)         2400000   \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 256)               571392    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8000)              2056000   \n",
      "=================================================================\n",
      "Total params: 5,027,392\n",
      "Trainable params: 5,027,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, CuDNNLSTM, Dropout, Dense\n",
    "\n",
    "hidden_dims = 256\n",
    "embed_dims = 300\n",
    "\n",
    "input_text = Input((None,))\n",
    "x = Embedding(num_vocabs, embed_dims)(input_text)\n",
    "x = CuDNNLSTM(hidden_dims)(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "output_text = Dense(num_vocabs, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(input_text, output_text)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=Adam(),\n",
    "             metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "418813/418813 [==============================] - 32s 76us/step - loss: 5.9349 - acc: 0.1178\n",
      "Epoch 2/50\n",
      "418813/418813 [==============================] - 28s 67us/step - loss: 4.9827 - acc: 0.1981\n",
      "Epoch 3/50\n",
      "418813/418813 [==============================] - 28s 67us/step - loss: 4.4842 - acc: 0.2476\n",
      "Epoch 4/50\n",
      "418813/418813 [==============================] - 28s 66us/step - loss: 4.1660 - acc: 0.2774\n",
      "Epoch 5/50\n",
      "418813/418813 [==============================] - 28s 66us/step - loss: 3.9094 - acc: 0.3003\n",
      "Epoch 6/50\n",
      "418813/418813 [==============================] - 28s 67us/step - loss: 3.7207 - acc: 0.3173\n",
      "Epoch 7/50\n",
      "418813/418813 [==============================] - 28s 68us/step - loss: 3.5447 - acc: 0.3338\n",
      "Epoch 8/50\n",
      "418813/418813 [==============================] - 28s 67us/step - loss: 3.3969 - acc: 0.3491\n",
      "Epoch 9/50\n",
      "418813/418813 [==============================] - 28s 67us/step - loss: 3.2609 - acc: 0.3646\n",
      "Epoch 10/50\n",
      "418813/418813 [==============================] - 28s 66us/step - loss: 3.1353 - acc: 0.3784\n",
      "Epoch 11/50\n",
      "418813/418813 [==============================] - 28s 67us/step - loss: 3.0176 - acc: 0.3941\n",
      "Epoch 12/50\n",
      "418813/418813 [==============================] - 28s 68us/step - loss: 2.9102 - acc: 0.4092\n",
      "Epoch 13/50\n",
      "418813/418813 [==============================] - 28s 67us/step - loss: 2.8077 - acc: 0.4226\n",
      "Epoch 14/50\n",
      "418813/418813 [==============================] - 28s 67us/step - loss: 2.7099 - acc: 0.4381\n",
      "Epoch 15/50\n",
      "418813/418813 [==============================] - 28s 68us/step - loss: 2.6179 - acc: 0.4516\n",
      "Epoch 16/50\n",
      "418813/418813 [==============================] - 28s 68us/step - loss: 2.5307 - acc: 0.4659\n",
      "Epoch 17/50\n",
      "418813/418813 [==============================] - 29s 69us/step - loss: 2.4477 - acc: 0.4798\n",
      "Epoch 18/50\n",
      "418813/418813 [==============================] - 29s 69us/step - loss: 2.3699 - acc: 0.4931\n",
      "Epoch 19/50\n",
      "418813/418813 [==============================] - 29s 69us/step - loss: 2.2962 - acc: 0.5065\n",
      "Epoch 20/50\n",
      "418813/418813 [==============================] - 28s 68us/step - loss: 2.2256 - acc: 0.5192\n",
      "Epoch 21/50\n",
      "418813/418813 [==============================] - 28s 67us/step - loss: 2.1592 - acc: 0.5311\n",
      "Epoch 22/50\n",
      "418813/418813 [==============================] - 29s 69us/step - loss: 2.0948 - acc: 0.5433\n",
      "Epoch 23/50\n",
      "418813/418813 [==============================] - 29s 68us/step - loss: 2.0366 - acc: 0.5540\n",
      "Epoch 24/50\n",
      "418813/418813 [==============================] - 28s 68us/step - loss: 1.9788 - acc: 0.5648\n",
      "Epoch 25/50\n",
      "418813/418813 [==============================] - 28s 67us/step - loss: 1.9259 - acc: 0.5750\n",
      "Epoch 26/50\n",
      "418813/418813 [==============================] - 28s 68us/step - loss: 1.8729 - acc: 0.5859\n",
      "Epoch 27/50\n",
      "418813/418813 [==============================] - 28s 68us/step - loss: 1.8257 - acc: 0.5944\n",
      "Epoch 28/50\n",
      "418813/418813 [==============================] - 28s 68us/step - loss: 1.7828 - acc: 0.6042\n",
      "Epoch 29/50\n",
      "418813/418813 [==============================] - 28s 67us/step - loss: 1.7397 - acc: 0.6117\n",
      "Epoch 30/50\n",
      "418813/418813 [==============================] - 29s 68us/step - loss: 1.6997 - acc: 0.6195\n",
      "Epoch 31/50\n",
      "418813/418813 [==============================] - 28s 67us/step - loss: 1.6622 - acc: 0.6269\n",
      "Epoch 32/50\n",
      "418813/418813 [==============================] - 28s 67us/step - loss: 1.6305 - acc: 0.6335\n",
      "Epoch 33/50\n",
      "418813/418813 [==============================] - 28s 67us/step - loss: 1.5891 - acc: 0.6416\n",
      "Epoch 34/50\n",
      "418813/418813 [==============================] - 28s 67us/step - loss: 1.5632 - acc: 0.6471\n",
      "Epoch 35/50\n",
      "418813/418813 [==============================] - 28s 66us/step - loss: 1.5343 - acc: 0.6519\n",
      "Epoch 36/50\n",
      "418813/418813 [==============================] - 28s 67us/step - loss: 1.5072 - acc: 0.6579\n",
      "Epoch 37/50\n",
      "418813/418813 [==============================] - 28s 67us/step - loss: 1.4790 - acc: 0.6634\n",
      "Epoch 38/50\n",
      "418813/418813 [==============================] - 28s 68us/step - loss: 1.4564 - acc: 0.6671\n",
      "Epoch 39/50\n",
      "418813/418813 [==============================] - 28s 68us/step - loss: 1.4337 - acc: 0.6719\n",
      "Epoch 40/50\n",
      "418813/418813 [==============================] - 28s 68us/step - loss: 1.4124 - acc: 0.6760\n",
      "Epoch 41/50\n",
      "418813/418813 [==============================] - 29s 68us/step - loss: 1.3914 - acc: 0.6795\n",
      "Epoch 42/50\n",
      "418813/418813 [==============================] - 29s 69us/step - loss: 1.3710 - acc: 0.6834\n",
      "Epoch 43/50\n",
      "418813/418813 [==============================] - 29s 69us/step - loss: 1.3541 - acc: 0.6869\n",
      "Epoch 44/50\n",
      "418813/418813 [==============================] - 28s 68us/step - loss: 1.3354 - acc: 0.6910\n",
      "Epoch 45/50\n",
      "418813/418813 [==============================] - 29s 68us/step - loss: 1.3202 - acc: 0.6935\n",
      "Epoch 46/50\n",
      "418813/418813 [==============================] - 29s 69us/step - loss: 1.3065 - acc: 0.6958\n",
      "Epoch 47/50\n",
      "418813/418813 [==============================] - 29s 69us/step - loss: 1.2923 - acc: 0.6991\n",
      "Epoch 48/50\n",
      "418813/418813 [==============================] - 29s 68us/step - loss: 1.2758 - acc: 0.7019\n",
      "Epoch 49/50\n",
      "418813/418813 [==============================] - 29s 69us/step - loss: 1.2687 - acc: 0.7027\n",
      "Epoch 50/50\n",
      "418813/418813 [==============================] - 29s 68us/step - loss: 1.2501 - acc: 0.7065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fffb02c2dd8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on \"temperature\" to alter a distribution\n",
    "A parameter called **\"temperature(softmax temperature)\"** is used to alter the original distribution below.\n",
    "<br>\n",
    "<br>\n",
    "$$\\exp(\\frac{\\log(original\\ distribution)}{temperature})$$\n",
    "$$= exp(\\log(original\\ distribution)) \\times exp(-temperature)$$\n",
    "$$= original\\ distribution \\times \\exp(-temperature)$$\n",
    "$$where\\ 0 \\neq temperature$$\n",
    "$$(use\\ argmax(original distribution)\\ where\\ temperature = 0)$$\n",
    "<br>\n",
    "That is, by multiplying original distribution by a number which is less than 1(equivalent to say temperature > 0), difference between high probabilities of some indexes and low probabilities of other indexes become small, so that the original distribution will have larger entropy(getting closer to uniform distribution, which is more unpredictable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alter_dist_and_pick_one(orig_dist, temp):\n",
    "    if temp == 0:\n",
    "        return np.argmax(orig_dist)\n",
    "    else:\n",
    "        dist = np.log(orig_dist + 1e-7) / temp\n",
    "        dist = np.exp(dist)\n",
    "        normalized_dist = dist / np.sum(dist)\n",
    "        choices = range(len(normalized_dist))\n",
    "        return np.random.choice(choices, p=normalized_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, max_len, temp=0.1):\n",
    "    generated_text = \"<【Sports Watch】日本を代表する打者である松井秀喜氏は\"\n",
    "    for i in range(max_len):\n",
    "        int_tokens = tokenizer.EncodeAsIds(generated_text)\n",
    "        #update int_tokens to predict next token\n",
    "        int_tokens = int_tokens[-seq_length:]\n",
    "        int_tokens = np.reshape(int_tokens, (1,seq_length))\n",
    "        pred_dist = model.predict(int_tokens, verbose=0)[0]\n",
    "        \n",
    "        pred_index = alter_dist_and_pick_one(pred_dist, temp)\n",
    "        generated_text += tokenizer.IdToPiece(int(pred_index))\n",
    "        \n",
    "        if generated_text[-1] == \">\" or generated_text[-3:] == \"▁▁▁\":\n",
    "            break\n",
    "        \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<【Sports Watch】日本を代表する打者である松井秀喜氏は「あんま怒りだな」「ジーコも時代遅れだ」「けは何なのか」と呟き、一転、自らが来いなければ中日であり、日本選手に加入して2シーズンを無失点の登板を生めぐる。▁▁今年は全国から45歳の武田修宏コーチを退任をまとめ役。欧州王者を更新するの日本男子17年ぶりの金メダル獲得に、9日に脳障害により引退危機「妊娠したら、20歳の女子高生チャンピオン」とも称される日々を買ってしまった。▁このニュースは複数のテレビ局の報道が流れ始めたことにも関わらず、この鼻水準を(担当者が公開パがいました)な。ミーティングした点と差があると思ってたら」と語った。▁▁▁\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(model, 1000, 0.3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
